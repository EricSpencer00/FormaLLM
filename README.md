# TLA+ LLM Research Pipeline

This project explores the use of Large Language Models (LLMs) to automatically generate formal specifications in [TLA+](https://lamport.azurewebsites.net/tla/tla.html) from natural language descriptions (comments). The generated specifications are syntactically validated using the TLA+ SANY parser and optionally executed with TLC.

---

## ðŸ“‚ Project Structure

```
FormaLLM/
â”œâ”€â”€ .devcontainer/        
â”‚   â””â”€â”€Dockerfile         # Custom environment with Java, Python, etc.
â”‚   â””â”€â”€devcontainer.json  # VS Code dev container config
â”œâ”€â”€ data/                 # Original and supporting data files
â”‚   â””â”€â”€ <model_name>/
â”‚       â”œâ”€â”€ txt/          # Natural language comments
â”‚       â”œâ”€â”€ tla/          # Ground-truth .tla files
â”‚       â””â”€â”€ cfg/          # Ground-truth .cfg files
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ evaluations/      # TLC logs
â”‚   â”œâ”€â”€ generated/        # LLM-generated .tla and .cfg files
â”‚   â”œâ”€â”€ test.json         # Testing metadata
â”‚   â”œâ”€â”€ train.json        # Training metadata
â”‚   â””â”€â”€ val.json          # Validation metadata
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ data_split_step.py         # Used for initial train, validation, test split
â”‚   â”œâ”€â”€ evaluate_generated_step.py # TLC evaluation 
â”‚   â”œâ”€â”€ graph_results.py.          # Results summar CSV + bar chart
â”‚   â”œâ”€â”€ parse_step.py              # SANY validation
â”‚   â”œâ”€â”€ prompt_step.py             # LangChain/LLM prompting logic
â”‚   â””â”€â”€ setup.sh.                  # Zenml + MLFlow setup script for experiments/logging
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ tla_pipeline.py   # Orchestrates full pipeline
â”œâ”€â”€ requirements.txt      # Python environment
â”œâ”€â”€ run.sh                # Interactive pipeline runner (recommended)
â”œâ”€â”€ run_pipeline.py       # ZenML pipeline runner with CLI args
â”œâ”€â”€ run_standalone.py     # Standalone runner (no ZenML, useful for compatibility)
â”œâ”€â”€ test_llm.py           # Quick LLM backend test script
â”œâ”€â”€ OLLAMA_MODELS.md      # Documentation for Ollama models
â”œâ”€â”€ mlruns/               # MLflow experiment logs (locally)
â””â”€â”€ .env                  # File containing API keys (not tracked in source control)
```

---

## ðŸ”§ Technologies and Tools

### LLM Backends (Configurable)
The pipeline supports multiple LLM backends:
- **OpenAI GPT-4** (via `langchain-openai`) - High-quality commercial API
- **Anthropic Claude** (via `langchain-anthropic`) - Alternative commercial API
- **Ollama** (via `langchain-ollama`) - Local/open-source models (llama3.1, codellama, deepseek-r1, etc.)

All backends are used interchangeably through the same LangChain interface.

### LangChain
- Handles the prompt logic and LLM chaining.
- Few-shot examples are embedded into prompts to improve generation quality.

### MLflow
- Logs all LLM traces, inputs/outputs, and artifacts.
- Tracks model performance over time.
- Automatically captures LangChain events using `mlflow.langchain.autolog()`.

### ZenML
- Orchestrates the full pipeline across modular steps.
- Handles reproducibility, caching, logging, and parameterization.

### TLC / SANY (Java)
- Validates the generated `.tla` files using the official TLA+ parser (SANY).
- Run through subprocess calls from within Python.

---

## Pipeline Overview

1. **Prompting Step (`prompt_step.py`)**
   - Loads training (few-shot) and validation data.
   - Builds prompts using comments and examples.
   - Sends the prompt to the LLM using LangChain.
   - Saves generated `.tla` and `.cfg` files under `outputs/generated/`.

2. **TLC Evaluation (`evaluate_generated_step.py`)**
   - Loads each generated `.tla` and `.cfg` file.
   - Runs the TLC via Java subprocess.
   - Logs `PASS`, `FAIL`, or `ERROR` status per file.

3. **Sanity Check Step (`parse_step.py`)**
   - Loads each generated `.tla` file.
   - Runs the SANY parser via Java subprocess.
   - Logs `PASS`, `FAIL`, or `ERROR` status per file.

4. **Tables and Graphs (`graph_results.py`)**
   -  Reads `evaluation_results.csv` from the `outputs/evaluations/` directory.
   - Counts the number of models with each result (`PASS`, `FAIL`, `ERROR`, etc.).
   - Saves Artifacts**:
       - `evaluation_summary.csv`: Tabular breakdown of results by type.
       - `evaluation_summary.png`: Bar chart of model validation outcomes.
   -  Prints the summary table directly to the console for quick insights.


5. **Pipeline Orchestration (`tla_pipeline.py`)**
   - Built using ZenMLâ€™s `@pipeline` decorator.
   - Chains the prompt and sanity steps.
   - Automatically tracks MLflow logs per run.

6. **Execution**

   **Option A: Interactive Script (Recommended)**
   ```bash
   ./run.sh
   ```
   - Select LLM backend (GPT-4, Claude, or Ollama)
   - Enter API keys for paid services (OpenAI/Anthropic)
   - Choose from available Ollama models
   - Select execution mode (ZenML orchestrated or standalone)

   **Option B: Direct Execution**
   ```bash
   # Using environment variables
   LLM_BACKEND=ollama LLM_MODEL=llama3.1 python run_standalone.py

   # Or with CLI arguments
   python run_standalone.py --backend ollama --model llama3.1

   # ZenML orchestrated pipeline
   python run_pipeline.py --backend openai --model gpt-4
   ```

---

## LLM Backend Configuration

### Supported Backends

#### 1. OpenAI (GPT-4)
```bash
export OPENAI_API_KEY="your-api-key-here"
./run.sh  # Select option 1
```

#### 2. Anthropic (Claude)
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
./run.sh  # Select option 2
```

#### 3. Ollama (Local Models)
```bash
# Install Ollama first: https://ollama.ai
ollama pull llama3.1  # or any other model
./run.sh  # Select option 3
```

**Popular Ollama Models:**
- `llama3.1` - 8B params, 4.7GB - Recommended for general use
- `codellama` - 7B params, 3.8GB - Code generation specialist
- `deepseek-r1` - 7B params, 4.7GB - Reasoning & code
- `qwq` - 32B params, 20GB - Advanced reasoning
- `phi4` - 14B params, 9.1GB - Microsoft flagship
- `mistral` - 7B params, 4.1GB - Fast & capable

See [OLLAMA_MODELS.md](OLLAMA_MODELS.md) for the complete list.

### API Key Management

**For paid APIs (OpenAI/Anthropic):**
1. The interactive script (`./run.sh`) will prompt for API keys
2. Or set environment variables:
   ```bash
   export OPENAI_API_KEY="sk-..."
   export ANTHROPIC_API_KEY="sk-ant-..."
   ```

**For Ollama:**
- No API key needed
- Runs 100% locally
- Requires Ollama to be installed and running

---

## Quick Start

### Using Ollama (Local, No API Key Required)
```bash
# 1. Install Ollama
# Visit https://ollama.ai

# 2. Pull a model
ollama pull llama3.1

# 3. Run the pipeline
./run.sh
# Select: 3 (Ollama) â†’ 1 (llama3.1) â†’ 2 (Standalone mode)
