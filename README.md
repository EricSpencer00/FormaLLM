# TLA+ LLM Research Pipeline

This project explores the use of Large Language Models (LLMs) to automatically generate formal specifications in [TLA+](https://lamport.azurewebsites.net/tla/tla.html) from natural language descriptions (comments). The generated specifications are syntactically validated using the TLA+ SANY parser and optionally executed with TLC.

---

## ðŸ“‚ Project Structure

```
FormaLLM/
â”œâ”€â”€ .devcontainer/        
â”‚   â””â”€â”€Dockerfile         # Custom environment with Java, Python, etc.
â”‚   â””â”€â”€devcontainer.json  # VS Code dev container config
â”œâ”€â”€ data/                 # Original and supporting data files
â”‚   â””â”€â”€ <model_name>/
â”‚       â”œâ”€â”€ txt/          # Natural language comments
â”‚       â”œâ”€â”€ tla/          # Ground-truth .tla files
â”‚       â””â”€â”€ cfg/          # Ground-truth .cfg files
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ evaluations/      # TLC logs
â”‚   â”œâ”€â”€ generated/        # LLM-generated .tla and .cfg files
â”‚   â”œâ”€â”€ test.json         # Testing metadata
â”‚   â”œâ”€â”€ train.json        # Training metadata
â”‚   â””â”€â”€ val.json          # Validation metadata
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ data_split_step.py         # Used for initial train, validation, test split
â”‚   â”œâ”€â”€ evaluate_generated_step.py # TLC evaluation 
â”‚   â”œâ”€â”€ graph_results.py.          # Results summar CSV + bar chart
â”‚   â”œâ”€â”€ parse_step.py              # SANY validation
â”‚   â”œâ”€â”€ prompt_step.py             # LangChain/LLM prompting logic
â”‚   â””â”€â”€ setup.sh.                  # Zenml + MLFlow setup script for experiments/logging
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ tla_pipeline.py   # Orchestrates full pipeline
â”œâ”€â”€ requirements.txt      # Python environment
â”œâ”€â”€ run_pipeline.py       # Entry point to launch the ZenML pipeline
â”œâ”€â”€ mlruns/               # MLflow experiment logs (locally)
â””â”€â”€ .env                  # File containing API keys (not tracked in source control)
```

---

## ðŸ”§ Technologies and Tools

### OpenAI API (via `langchain-openai`)
- Powers the LLM prompting for TLA+ generation.
- Uses `gpt-4` to generate valid TLA+ specs and optional TLC configs.

### LangChain
- Handles the prompt logic and LLM chaining.
- Few-shot examples are embedded into prompts to improve generation quality.

### MLflow
- Logs all LLM traces, inputs/outputs, and artifacts.
- Tracks model performance over time.
- Automatically captures LangChain events using `mlflow.langchain.autolog()`.

### ZenML
- Orchestrates the full pipeline across modular steps.
- Handles reproducibility, caching, logging, and parameterization.

### TLC / SANY (Java)
- Validates the generated `.tla` files using the official TLA+ parser (SANY).
- Run through subprocess calls from within Python.

---

## Pipeline Overview

1. **Prompting Step (`prompt_step.py`)**
   - Loads training (few-shot) and validation data.
   - Builds prompts using comments and examples.
   - Sends the prompt to the LLM using LangChain.
   - Saves generated `.tla` and `.cfg` files under `outputs/generated/`.

2. **TLC Evaluation (`evaluate_generated_step.py`)**
   - Loads each generated `.tla` and `.cfg` file.
   - Runs the TLC via Java subprocess.
   - Logs `PASS`, `FAIL`, or `ERROR` status per file.

3. **Sanity Check Step (`parse_step.py`)**
   - Loads each generated `.tla` file.
   - Runs the SANY parser via Java subprocess.
   - Logs `PASS`, `FAIL`, or `ERROR` status per file.

4. **Tables and Graphs (`graph_results.py`)**
   -  Reads `evaluation_results.csv` from the `outputs/evaluations/` directory.
   - Counts the number of models with each result (`PASS`, `FAIL`, `ERROR`, etc.).
   - Saves Artifacts**:
       - `evaluation_summary.csv`: Tabular breakdown of results by type.
       - `evaluation_summary.png`: Bar chart of model validation outcomes.
   -  Prints the summary table directly to the console for quick insights.


5. **Pipeline Orchestration (`tla_pipeline.py`)**
   - Built using ZenMLâ€™s `@pipeline` decorator.
   - Chains the prompt and sanity steps.
   - Automatically tracks MLflow logs per run.

6. **Execution**
   ```bash
   python run_pipeline.py
