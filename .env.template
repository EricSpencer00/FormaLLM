# FormaLLM Environment Configuration Template
# Copy this file to .env and fill in your actual values
# Generated by FormaLLM setup script

# =============================================================================
# LLM Backend Configuration
# =============================================================================

# Primary LLM backend to use (openai|anthropic|ollama)
LLM_BACKEND=ollama

# Current model for the selected backend
LLM_MODEL=llama3.1

# =============================================================================
# OpenAI Configuration
# =============================================================================

# OpenAI API key - get from https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-key-here

# OpenAI model to use (gpt-4, gpt-4-turbo, gpt-3.5-turbo, etc.)
OPENAI_MODEL=gpt-4

# Optional: OpenAI organization ID
# OPENAI_ORG_ID=your-org-id

# =============================================================================
# Anthropic Configuration
# =============================================================================

# Anthropic API key - get from https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-key-here

# Claude model to use
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# =============================================================================
# Ollama Configuration (Local LLM)
# =============================================================================

# Enable/disable Ollama service
OLLAMA_ENABLED=true

# Ollama base URL
# - For native installation: http://localhost:11434
# - For Docker Compose: http://localhost:11435
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use
OLLAMA_MODEL=llama3.1

# =============================================================================
# Alternative Local LLM Services
# =============================================================================

# LocalAI configuration (OpenAI-compatible local API)
# LOCALAI_BASE_URL=http://localhost:8080
# LOCALAI_MODEL=your-local-model

# Text Generation WebUI configuration
# WEBUI_BASE_URL=http://localhost:5000
# WEBUI_MODEL=your-webui-model

# =============================================================================
# MLflow Configuration
# =============================================================================

# MLflow tracking URI for experiment logging
MLFLOW_TRACKING_URI=file:///workspaces/FormaLLM/mlruns

# Optional: Remote MLflow server
# MLFLOW_TRACKING_URI=http://localhost:5001

# MLflow experiment name
MLFLOW_EXPERIMENT_NAME=tla_prompt_generation

# =============================================================================
# TLA+ Tools Configuration
# =============================================================================

# Path to TLA+ tools directory
TLA_TOOLS_DIR=/opt/tla

# Java options for TLA+ tools (optional)
# TLA_JAVA_OPTS=-Xmx4g

# =============================================================================
# Pipeline Configuration
# =============================================================================

# Number of few-shot examples to use in prompts
NUM_FEW_SHOTS=3

# Timeout for TLA+ tool executions (seconds)
TLA_TIMEOUT=30

# Enable/disable pipeline caching
ENABLE_CACHE=true

# =============================================================================
# Development & Debugging
# =============================================================================

# Enable debug logging
DEBUG=false

# Python log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Disable warnings for cleaner output
PYTHONWARNINGS=ignore

# =============================================================================
# Docker Configuration
# =============================================================================

# Docker Compose profiles to enable
# Available profiles: ollama, localai, webui, mlflow, cache, advanced
COMPOSE_PROFILES=ollama

# Docker network name
COMPOSE_PROJECT_NAME=formaLLM

# =============================================================================
# Advanced Configuration
# =============================================================================

# Redis configuration (for caching)
# REDIS_URL=redis://localhost:6379

# Custom model configurations (JSON format)
# CUSTOM_MODEL_CONFIG={"temperature": 0.1, "max_tokens": 2048}

# Batch processing settings
# BATCH_SIZE=10
# MAX_CONCURRENT_REQUESTS=5

# =============================================================================
# Security Notes
# =============================================================================
# 
# 1. Never commit this file with real API keys to version control
# 2. Use strong, unique API keys for production environments  
# 3. Consider using environment-specific .env files (.env.dev, .env.prod)
# 4. For CI/CD, use secure environment variable injection
# 5. Regularly rotate API keys and monitor usage